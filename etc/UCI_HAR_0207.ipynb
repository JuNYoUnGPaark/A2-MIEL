{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVpPI87NqwNu",
        "outputId": "931d6195-787c-4f33-c7d7-dd73285b5b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "MIEL-HAR with Attention-to-Attention (A²)\n",
            "================================================================================\n",
            "Device: cuda\n",
            "\n",
            "[train] X: (7352, 128, 9), y: (7352,)\n",
            "[test] X: (2947, 128, 9), y: (2947,)\n",
            "Parameters: 71,015 (0.071 M)\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch    LR         Total      Cls        E          TestF1(macro)  Best  \n",
            "--------------------------------------------------------------------------------\n",
            "10       0.000500   0.3424     0.3268     0.1558     0.9446         *     \n",
            "20       0.000500   0.3079     0.2947     0.1324     0.9423               \n",
            "30       0.000500   0.2878     0.2756     0.1222     0.9354               \n",
            "40       0.000500   0.2722     0.2614     0.1074     0.9528               \n",
            "50       0.000500   0.2665     0.2567     0.0974     0.9517               \n",
            "60       0.000500   0.2644     0.2555     0.0887     0.9536               \n",
            "70       0.000500   0.2668     0.2585     0.0825     0.9505               \n",
            "80       0.000500   0.2566     0.2495     0.0710     0.9565               \n",
            "90       0.000500   0.2563     0.2492     0.0710     0.9551               \n",
            "100      0.000500   0.2608     0.2543     0.0654     0.9458               \n",
            "--------------------------------------------------------------------------------\n",
            "Best Test F1(macro): 0.9605 @ epoch 88\n",
            "\n",
            "Final Test (Detailed):\n",
            "\n",
            "[Macro-F1] 0.9605\n",
            "\n",
            "[Classification Report]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        WALK     0.9860    0.9919    0.9889       496\n",
            "     WALK_UP     0.9486    0.9788    0.9634       471\n",
            "   WALK_DOWN     0.9876    0.9452    0.9659       420\n",
            "         SIT     0.9501    0.8921    0.9202       491\n",
            "       STAND     0.9104    0.9549    0.9321       532\n",
            "         LAY     0.9889    0.9963    0.9926       537\n",
            "\n",
            "    accuracy                         0.9606      2947\n",
            "   macro avg     0.9619    0.9599    0.9605      2947\n",
            "weighted avg     0.9611    0.9606    0.9605      2947\n",
            "\n",
            "[Confusion Matrix] (rows=true, cols=pred)\n",
            "[[492   2   2   0   0   0]\n",
            " [  7 461   3   0   0   0]\n",
            " [  0  23 397   0   0   0]\n",
            " [  0   0   0 438  49   4]\n",
            " [  0   0   0  22 508   2]\n",
            " [  0   0   0   1   1 535]]\n",
            "\n",
            "Final Macro-F1: 0.9605\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"369\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 0) Utils\n",
        "# ------------------------------------------------------------------------------\n",
        "def set_seed(seed: int, deterministic: bool = True):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.use_deterministic_algorithms(True, warn_only=False)\n",
        "\n",
        "\n",
        "def make_seed_worker(seed: int):\n",
        "    def seed_worker(worker_id: int):\n",
        "        worker_seed = seed + worker_id\n",
        "        np.random.seed(worker_seed)\n",
        "        random.seed(worker_seed)\n",
        "    return seed_worker\n",
        "\n",
        "\n",
        "def count_params(model, trainable_only=True):\n",
        "    if trainable_only:\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1) Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_path, split='train'):\n",
        "        self.split = split\n",
        "\n",
        "        if split == 'train':\n",
        "            y = np.loadtxt(os.path.join(data_path, 'train', 'y_train.txt'))\n",
        "            signal_path = os.path.join(data_path, 'train', 'Inertial Signals')\n",
        "        else:\n",
        "            y = np.loadtxt(os.path.join(data_path, 'test', 'y_test.txt'))\n",
        "            signal_path = os.path.join(data_path, 'test', 'Inertial Signals')\n",
        "\n",
        "        signals = []\n",
        "        signal_files = [\n",
        "            \"total_acc_x\", \"total_acc_y\", \"total_acc_z\",\n",
        "            \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "            \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "        ]\n",
        "\n",
        "        for signal_file in signal_files:\n",
        "            filename = os.path.join(signal_path, f'{signal_file}_{split}.txt')\n",
        "            signals.append(np.loadtxt(filename))\n",
        "\n",
        "        # X: (N, T, C)\n",
        "        self.X = np.stack(signals, axis=-1).astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64)\n",
        "\n",
        "        print(f\"[{split}] X: {self.X.shape}, y: {self.y.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx]).float()     # (T, C)\n",
        "        x = x.permute(1, 0).contiguous()              # (C, T)\n",
        "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def fit_transform_scaler_train_only(train_X, test_X, gravity_scale=10.0):\n",
        "    N, C, T = train_X.shape\n",
        "    N_test = test_X.shape[0]\n",
        "\n",
        "    train_data = train_X.transpose(0, 2, 1)\n",
        "    test_data  = test_X.transpose(0, 2, 1)\n",
        "\n",
        "    train_scaled = np.zeros_like(train_data)\n",
        "    test_scaled  = np.zeros_like(test_data)\n",
        "\n",
        "    acc_idx = [0, 1, 2]\n",
        "\n",
        "    train_scaled[:, :, acc_idx] = train_data[:, :, acc_idx] / gravity_scale\n",
        "    test_scaled[:, :, acc_idx]  = test_data[:, :, acc_idx]  / gravity_scale\n",
        "\n",
        "    rest_idx = [3, 4, 5, 6, 7, 8]\n",
        "\n",
        "    train_rest_flat = train_data[:, :, rest_idx].reshape(-1, len(rest_idx))\n",
        "    test_rest_flat  = test_data[:, :, rest_idx].reshape(-1, len(rest_idx))\n",
        "\n",
        "    scaler_rest = RobustScaler(quantile_range=(25.0, 75.0))\n",
        "    scaler_rest.fit(train_rest_flat)\n",
        "\n",
        "    train_scaled[:, :, rest_idx] = scaler_rest.transform(train_rest_flat).reshape(N, T, len(rest_idx))\n",
        "    test_scaled[:, :, rest_idx]  = scaler_rest.transform(test_rest_flat).reshape(N_test, T, len(rest_idx))\n",
        "\n",
        "    return train_scaled.transpose(0, 2, 1).astype(np.float32), test_scaled.transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "\n",
        "class ArrayDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()   # (N, C, T)\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2) Model\n",
        "# ------------------------------------------------------------------------------\n",
        "class RelativeEnergyPhysics(nn.Module):\n",
        "    def __init__(self, acc_indices, gyro_indices):\n",
        "        super().__init__()\n",
        "        self.acc_indices = acc_indices\n",
        "        self.gyro_indices = gyro_indices\n",
        "        self.m = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
        "        self.I = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):  # x: (B, C, T)\n",
        "        m_pos = F.softplus(self.m)\n",
        "        I_pos = F.softplus(self.I)\n",
        "\n",
        "        x_t = x.transpose(1, 2)  # (B, T, C)\n",
        "        B, T, _ = x_t.shape\n",
        "\n",
        "        # accel proxy\n",
        "        if len(self.acc_indices) > 0:\n",
        "            acc_data = x_t[:, :, self.acc_indices]\n",
        "            n_acc = len(self.acc_indices) // 3\n",
        "            if n_acc > 0:\n",
        "                acc_reshaped = acc_data.view(B, T, n_acc, 3)\n",
        "                acc_mag = (acc_reshaped ** 2).sum(dim=-1).mean(dim=-1, keepdim=True)  # (B,T,1)\n",
        "            else:\n",
        "                acc_mag = torch.zeros(B, T, 1, device=x.device)\n",
        "        else:\n",
        "            acc_mag = torch.zeros(B, T, 1, device=x.device)\n",
        "\n",
        "        E_kin = 0.5 * m_pos * acc_mag\n",
        "\n",
        "        # gyro proxy\n",
        "        if len(self.gyro_indices) > 0:\n",
        "            gyro_data = x_t[:, :, self.gyro_indices]\n",
        "            n_gyro = len(self.gyro_indices) // 3\n",
        "            if n_gyro > 0:\n",
        "                gyro_reshaped = gyro_data.view(B, T, n_gyro, 3)\n",
        "                gyro_mag = (gyro_reshaped ** 2).sum(dim=-1).mean(dim=-1, keepdim=True)  # (B,T,1)\n",
        "            else:\n",
        "                gyro_mag = torch.zeros(B, T, 1, device=x.device)\n",
        "        else:\n",
        "            gyro_mag = torch.zeros(B, T, 1, device=x.device)\n",
        "\n",
        "        E_rot = 0.5 * I_pos * gyro_mag\n",
        "        return E_kin + E_rot  # (B,T,1)\n",
        "\n",
        "\n",
        "class PotentialEnergyField(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.energy_net = nn.Sequential(\n",
        "            nn.Linear(input_dim + 1, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, E_phys):  # x:(B,C,T), E_phys:(B,T,1)\n",
        "        x_t = x.transpose(1, 2)                # (B,T,C)\n",
        "        x_aug = torch.cat([x_t, E_phys], -1)   # (B,T,C+1)\n",
        "        return self.energy_net(x_aug)          # (B,T,1)\n",
        "\n",
        "\n",
        "class EnergyGradientFlow(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.gradient_net = nn.Sequential(\n",
        "            nn.Linear(input_dim + 1, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, E_phys):  # x:(B,C,T), E_phys:(B,T,1)\n",
        "        x_t = x.transpose(1, 2)                # (B,T,C)\n",
        "        x_aug = torch.cat([x_t, E_phys], -1)   # (B,T,C+1)\n",
        "        return self.gradient_net(x_aug)        # (B,T,C)\n",
        "\n",
        "\n",
        "class RateAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.rate_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.rate_gate = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, features, energy):  # features:(B,T,H), energy:(B,T,1)\n",
        "        energy_rate = torch.cat([energy[:, :1, :], energy[:, 1:, :] - energy[:, :-1, :]], dim=1)\n",
        "        ctx = self.rate_proj(features)\n",
        "        scores = self.rate_gate(ctx + energy_rate)\n",
        "        attn = torch.softmax(scores.squeeze(-1), dim=1)\n",
        "        return attn, features * attn.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.phase_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.phase_gate = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, features, energy):\n",
        "        energy_pad = F.pad(energy.transpose(1, 2), (1, 1), mode='replicate').transpose(1, 2)\n",
        "        curv = energy_pad[:, 2:, :] - 2 * energy_pad[:, 1:-1, :] + energy_pad[:, :-2, :]\n",
        "        ctx = self.phase_proj(features)\n",
        "        scores = self.phase_gate(ctx + curv)\n",
        "        attn = torch.softmax(scores.squeeze(-1), dim=1)\n",
        "        return attn, features * attn.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class AttentionToAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.meta_attention = nn.Sequential(\n",
        "            nn.Linear(2, hidden_dim // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim // 4, 2),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.conflict_detector = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, rate_attn, phase_attn, rate_feat, phase_feat):\n",
        "        attn_stack = torch.stack([rate_attn, phase_attn], dim=-1)  # (B,T,2)\n",
        "        reliability = self.meta_attention(attn_stack)              # (B,T,2)\n",
        "        rate_reliability = reliability[:, :, 0:1]\n",
        "        phase_reliability = reliability[:, :, 1:2]\n",
        "\n",
        "        feat_cat = torch.cat([rate_feat, phase_feat], dim=-1)\n",
        "        conflict_score = self.conflict_detector(feat_cat).mean(dim=1).squeeze(-1)\n",
        "\n",
        "        reconciled = rate_reliability * rate_feat + phase_reliability * phase_feat\n",
        "        return reconciled, conflict_score\n",
        "\n",
        "\n",
        "class LandscapeGeometryEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        combined_dim = input_dim * 2 + 1\n",
        "        self.input_proj = nn.Conv1d(combined_dim, hidden_dim, kernel_size=1)\n",
        "        self.conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.temporal_attn = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
        "\n",
        "        self.rate_attention = RateAttention(hidden_dim)\n",
        "        self.phase_attention = PhaseAttention(hidden_dim)\n",
        "        self.a2 = AttentionToAttention(hidden_dim)\n",
        "\n",
        "    def forward(self, energy, gradient, x_original):\n",
        "        x_t = x_original.transpose(1, 2)                     # (B,T,C)\n",
        "        state = torch.cat([x_t, gradient, energy], dim=-1)   # (B,T,2C+1)\n",
        "\n",
        "        h = self.input_proj(state.transpose(1, 2))           # (B,H,T)\n",
        "        h = self.conv(h)\n",
        "        h = self.norm(h.transpose(1, 2)).transpose(1, 2)\n",
        "        h = F.gelu(h)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        h_t = h.transpose(1, 2)                              # (B,T,H)\n",
        "        h_t, _ = self.temporal_attn(h_t, h_t, h_t)\n",
        "\n",
        "        rate_attn, rate_feat = self.rate_attention(h_t, energy)\n",
        "        phase_attn, phase_feat = self.phase_attention(h_t, energy)\n",
        "        reconciled, conflict = self.a2(rate_attn, phase_attn, rate_feat, phase_feat)\n",
        "        return reconciled, conflict\n",
        "\n",
        "\n",
        "class MIELHAR_A2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, sensor_config,\n",
        "                 physics_grad_weight=0.1, energy_reg_weight=0.01):\n",
        "        super().__init__()\n",
        "        self.acc_indices = sensor_config[\"acc_indices\"]\n",
        "        self.gyro_indices = sensor_config[\"gyro_indices\"]\n",
        "\n",
        "        self.physics = RelativeEnergyPhysics(self.acc_indices, self.gyro_indices)\n",
        "        self.potential_field = PotentialEnergyField(input_dim, hidden_dim)\n",
        "        self.gradient_flow   = EnergyGradientFlow(input_dim, hidden_dim)\n",
        "        self.physics_grad_weight = physics_grad_weight\n",
        "\n",
        "        self.encoder = LandscapeGeometryEncoder(input_dim, hidden_dim)\n",
        "        self.energy_reg_weight = energy_reg_weight\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def _physics_grad(self, x):\n",
        "        x_t = x.transpose(1, 2)  # (B,T,C)\n",
        "        m_pos = F.softplus(self.physics.m)\n",
        "        I_pos = F.softplus(self.physics.I)\n",
        "\n",
        "        pg = torch.zeros_like(x_t)\n",
        "        if len(self.acc_indices) > 0:\n",
        "            pg[:, :, self.acc_indices] = x_t[:, :, self.acc_indices] * m_pos\n",
        "        if len(self.gyro_indices) > 0:\n",
        "            pg[:, :, self.gyro_indices] = x_t[:, :, self.gyro_indices] * I_pos\n",
        "        return pg\n",
        "\n",
        "    def forward(self, x, return_energy=False):\n",
        "        E_phys = self.physics(x)  # (B,T,1)\n",
        "\n",
        "        energy = self.potential_field(x, E_phys)        # (B,T,1)\n",
        "        grad   = self.gradient_flow(x, E_phys)          # (B,T,C)\n",
        "        grad = grad + self.physics_grad_weight * self._physics_grad(x)\n",
        "\n",
        "        landscape_features, _ = self.encoder(energy, grad, x)  # (B,T,H)\n",
        "\n",
        "        g = self.pool(landscape_features.transpose(1, 2)).squeeze(-1)  # (B,H)\n",
        "        logits = self.classifier(g)\n",
        "\n",
        "        if return_energy:\n",
        "            return logits, energy, grad\n",
        "        return logits\n",
        "\n",
        "\n",
        "def compute_energy_loss(energy, gradient, beta_gradmag=0.1):\n",
        "    energy_diff = energy[:, 1:] - energy[:, :-1]\n",
        "    smoothness = torch.mean(energy_diff ** 2)\n",
        "    grad_mag = torch.norm(gradient, dim=-1)\n",
        "    grad_loss = torch.mean(grad_mag)\n",
        "    return smoothness + beta_gradmag * grad_loss\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3) Train/Eval\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, beta_gradmag=0.1):\n",
        "    model.train()\n",
        "\n",
        "    total_sum = 0.0\n",
        "    cls_sum = 0.0\n",
        "    e_sum = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        logits, energy, grad = model(x, return_energy=True)\n",
        "        cls_loss = criterion(logits, y)\n",
        "        e_loss = compute_energy_loss(energy, grad, beta_gradmag=beta_gradmag)\n",
        "\n",
        "        loss = cls_loss + model.energy_reg_weight * e_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_sum += loss.item()\n",
        "        cls_sum += cls_loss.item()\n",
        "        e_sum += e_loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return {\n",
        "        \"loss_total\": total_sum / max(1, n_batches),\n",
        "        \"loss_cls\": cls_sum / max(1, n_batches),\n",
        "        \"loss_energy\": e_sum / max(1, n_batches),\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_metrics(model, loader, device, class_names=None, verbose=False):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(y.numpy())\n",
        "\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_true = np.concatenate(all_labels)\n",
        "\n",
        "    macro = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    if verbose:\n",
        "        if class_names is None:\n",
        "            class_names = [f\"class{i}\" for i in range(len(np.unique(y_true)))]\n",
        "        print(f\"\\n[Macro-F1] {macro:.4f}\\n\")\n",
        "        print(\"[Classification Report]\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "        print(\"[Confusion Matrix] (rows=true, cols=pred)\")\n",
        "        print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    return macro\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4) Main\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    CFG = {\n",
        "        \"seed\": 369,\n",
        "        \"deterministic\": True,\n",
        "\n",
        "        \"dataset_root\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/UCI_HAR\",\n",
        "\n",
        "        \"batch_size\": 64,\n",
        "        \"num_workers\": 2,\n",
        "        \"pin_memory\": True,\n",
        "\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"weight_decay\": 3e-4,\n",
        "        \"label_smoothing\": 0.05,\n",
        "        \"physics_grad_weight\": 0.25,  # EnergyGradientFlow: grad + alpha * physics_grad_weight\n",
        "\n",
        "        \"acc_indices\": [0, 1, 2],\n",
        "        \"gyro_indices\": [6, 7, 8],\n",
        "\n",
        "        # loss weights\n",
        "        \"w_energy_reg\": 0.1,      # multiply energy loss\n",
        "        \"beta_gradmag\": 0.2,      # compute_energy_loss: smooth + beta * gradmag\n",
        "\n",
        "        # model\n",
        "        \"hidden_dim\": 64,\n",
        "        \"num_classes\": 6,\n",
        "        \"input_dim\": 9,\n",
        "\n",
        "        # logging\n",
        "        \"log_every\": 10,\n",
        "    }\n",
        "\n",
        "    set_seed(CFG[\"seed\"], CFG[\"deterministic\"])\n",
        "    seed_worker = make_seed_worker(CFG[\"seed\"])\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(CFG[\"seed\"])\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"MIEL-HAR with Attention-to-Attention (A²)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Device: {device}\\n\")\n",
        "\n",
        "    # 1) Load official splits\n",
        "    train_raw = UCIHARDataset(CFG[\"dataset_root\"], split=\"train\")\n",
        "    test_raw  = UCIHARDataset(CFG[\"dataset_root\"], split=\"test\")\n",
        "\n",
        "    X_train = np.transpose(train_raw.X, (0, 2, 1)).astype(np.float32)  # (N,C,T)\n",
        "    y_train = train_raw.y.astype(np.int64)\n",
        "\n",
        "    X_test = np.transpose(test_raw.X, (0, 2, 1)).astype(np.float32)\n",
        "    y_test = test_raw.y.astype(np.int64)\n",
        "\n",
        "    # 2) Standardize train-only\n",
        "    X_train_s, X_test_s = fit_transform_scaler_train_only(X_train, X_test)\n",
        "\n",
        "    train_ds = ArrayDataset(X_train_s, y_train)\n",
        "    test_ds  = ArrayDataset(X_test_s, y_test)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=CFG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=CFG[\"num_workers\"],\n",
        "        pin_memory=CFG[\"pin_memory\"] and (device.type == \"cuda\"),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        persistent_workers=(CFG[\"num_workers\"] > 0),\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=CFG[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=CFG[\"num_workers\"],\n",
        "        pin_memory=CFG[\"pin_memory\"] and (device.type == \"cuda\"),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        persistent_workers=(CFG[\"num_workers\"] > 0),\n",
        "    )\n",
        "\n",
        "    sensor_config = {\"acc_indices\": CFG[\"acc_indices\"], \"gyro_indices\": CFG[\"gyro_indices\"]}\n",
        "\n",
        "    model = MIELHAR_A2(\n",
        "        input_dim=CFG[\"input_dim\"],\n",
        "        hidden_dim=CFG[\"hidden_dim\"],\n",
        "        num_classes=CFG[\"num_classes\"],\n",
        "        sensor_config=sensor_config,\n",
        "        physics_grad_weight=CFG[\"physics_grad_weight\"],\n",
        "        energy_reg_weight=CFG[\"w_energy_reg\"],\n",
        "    ).to(device)\n",
        "\n",
        "    n_params = count_params(model, trainable_only=True)\n",
        "    print(f\"Parameters: {n_params:,} ({n_params/1e6:.3f} M)\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
        "\n",
        "    warmup_epochs = 5\n",
        "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=0.1,\n",
        "        end_factor=1.0,\n",
        "        total_iters=warmup_epochs\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=CFG[\"label_smoothing\"])\n",
        "\n",
        "    best_test_f1 = -1.0\n",
        "    best_state = None\n",
        "    best_epoch = -1\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Epoch':<8} {'LR':<10} {'Total':<10} {'Cls':<10} {'E':<10} {'TestF1(macro)':<14} {'Best':<6}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for epoch in range(1, CFG[\"epochs\"] + 1):\n",
        "        train_logs = train_one_epoch(\n",
        "            model=model,\n",
        "            loader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            beta_gradmag=CFG[\"beta_gradmag\"],\n",
        "        )\n",
        "\n",
        "        if epoch <= warmup_epochs:\n",
        "            warmup_scheduler.step()\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        test_f1 = eval_metrics(model, test_loader, device, verbose=False)\n",
        "\n",
        "        is_best = \"\"\n",
        "        if test_f1 > best_test_f1 + 1e-12:\n",
        "            best_test_f1 = test_f1\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            best_epoch = epoch\n",
        "            is_best = \"*\"\n",
        "\n",
        "        if (epoch % CFG[\"log_every\"]) == 0:\n",
        "            print(f\"{epoch:<8} {lr:<10.6f} \"\n",
        "                  f\"{train_logs['loss_total']:<10.4f} {train_logs['loss_cls']:<10.4f} {train_logs['loss_energy']:<10.4f} \"\n",
        "                  f\"{test_f1:<14.4f} {is_best:<6}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Best Test F1(macro): {best_test_f1:.4f} @ epoch {best_epoch}\")\n",
        "\n",
        "    # final confirm\n",
        "    model.load_state_dict(best_state)\n",
        "    print(\"\\nFinal Test (Detailed):\")\n",
        "    class_names = [\"WALK\", \"WALK_UP\", \"WALK_DOWN\", \"SIT\", \"STAND\", \"LAY\"]\n",
        "    final_test_f1 = eval_metrics(model, test_loader, device, class_names=class_names, verbose=True)\n",
        "    print(f\"\\nFinal Macro-F1: {final_test_f1:.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}