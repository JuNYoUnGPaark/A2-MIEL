{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVpPI87NqwNu",
        "outputId": "6060f0db-4943-46ce-f151-6a713b8073ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "MIEL-HAR with Attention-to-Attention (A²)\n",
            "================================================================================\n",
            "Device: cuda\n",
            "\n",
            "[train] X: (7352, 128, 9), y: (7352,)\n",
            "[test] X: (2947, 128, 9), y: (2947,)\n",
            "Parameters: 71,015 (0.071 M)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch    LR         Total      Cls        E          TestF1(macro)  Best  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "10       0.000800   0.4275     0.4054     0.2216     0.9278         *     \n",
            "20       0.000800   0.3005     0.2865     0.1400     0.9386               \n",
            "30       0.000800   0.2807     0.2685     0.1225     0.9434               \n",
            "40       0.000800   0.2656     0.2547     0.1086     0.9423               \n",
            "50       0.000800   0.2711     0.2610     0.1008     0.9400               \n",
            "60       0.000800   0.2567     0.2476     0.0913     0.9464               \n",
            "70       0.000800   0.2557     0.2471     0.0860     0.9497               \n",
            "80       0.000800   0.2565     0.2486     0.0793     0.9438               \n",
            "90       0.000800   0.2539     0.2463     0.0759     0.9457               \n",
            "100      0.000800   0.2511     0.2443     0.0682     0.9503               \n",
            "--------------------------------------------------------------------------------\n",
            "Best Test F1(macro): 0.9551 @ epoch 61\n",
            "\n",
            "Final Test:\n",
            "Test F1(macro): 0.9551\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 0) Utils\n",
        "# ------------------------------------------------------------------------------\n",
        "def set_seed(seed: int, deterministic: bool = True):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.use_deterministic_algorithms(True, warn_only=False)\n",
        "\n",
        "\n",
        "def make_seed_worker(seed: int):\n",
        "    def seed_worker(worker_id: int):\n",
        "        worker_seed = seed + worker_id\n",
        "        np.random.seed(worker_seed)\n",
        "        random.seed(worker_seed)\n",
        "    return seed_worker\n",
        "\n",
        "\n",
        "def count_params(model, trainable_only=True):\n",
        "    if trainable_only:\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1) Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_path, split='train'):\n",
        "        self.split = split\n",
        "\n",
        "        if split == 'train':\n",
        "            y = np.loadtxt(os.path.join(data_path, 'train', 'y_train.txt'))\n",
        "            signal_path = os.path.join(data_path, 'train', 'Inertial Signals')\n",
        "        else:\n",
        "            y = np.loadtxt(os.path.join(data_path, 'test', 'y_test.txt'))\n",
        "            signal_path = os.path.join(data_path, 'test', 'Inertial Signals')\n",
        "\n",
        "        signals = []\n",
        "        signal_files = [\n",
        "            \"total_acc_x\", \"total_acc_y\", \"total_acc_z\",\n",
        "            \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "            \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "        ]\n",
        "\n",
        "        for signal_file in signal_files:\n",
        "            filename = os.path.join(signal_path, f'{signal_file}_{split}.txt')\n",
        "            signals.append(np.loadtxt(filename))\n",
        "\n",
        "        # X: (N, T, C)\n",
        "        self.X = np.stack(signals, axis=-1).astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64)\n",
        "\n",
        "        print(f\"[{split}] X: {self.X.shape}, y: {self.y.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx]).float()     # (T, C)\n",
        "        x = x.permute(1, 0).contiguous()              # (C, T)\n",
        "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def fit_transform_scaler_train_only(train_X, test_X):\n",
        "    N, C, T = train_X.shape\n",
        "\n",
        "    train_flat = train_X.transpose(0, 2, 1).reshape(-1, C)  # (N*T, C)\n",
        "    test_flat  = test_X.transpose(0, 2, 1).reshape(-1, C)\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    scaler.fit(train_flat)\n",
        "\n",
        "    train_scaled = scaler.transform(train_flat).reshape(N, T, C).transpose(0, 2, 1)\n",
        "    test_scaled  = scaler.transform(test_flat).reshape(test_X.shape[0], T, C).transpose(0, 2, 1)\n",
        "\n",
        "    return train_scaled.astype(np.float32), test_scaled.astype(np.float32)\n",
        "\n",
        "\n",
        "class ArrayDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()   # (N, C, T)\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2) Model\n",
        "# ------------------------------------------------------------------------------\n",
        "class RelativeEnergyPhysics(nn.Module):\n",
        "    def __init__(self, acc_indices, gyro_indices):\n",
        "        super().__init__()\n",
        "        self.acc_indices = acc_indices\n",
        "        self.gyro_indices = gyro_indices\n",
        "        self.m = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
        "        self.I = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):  # x: (B, C, T)\n",
        "        m_pos = F.softplus(self.m)\n",
        "        I_pos = F.softplus(self.I)\n",
        "\n",
        "        x_t = x.transpose(1, 2)  # (B, T, C)\n",
        "        B, T, _ = x_t.shape\n",
        "\n",
        "        # accel proxy\n",
        "        if len(self.acc_indices) > 0:\n",
        "            acc_data = x_t[:, :, self.acc_indices]\n",
        "            n_acc = len(self.acc_indices) // 3\n",
        "            if n_acc > 0:\n",
        "                acc_reshaped = acc_data.view(B, T, n_acc, 3)\n",
        "                acc_mag = (acc_reshaped ** 2).sum(dim=-1).mean(dim=-1, keepdim=True)  # (B,T,1)\n",
        "            else:\n",
        "                acc_mag = torch.zeros(B, T, 1, device=x.device)\n",
        "        else:\n",
        "            acc_mag = torch.zeros(B, T, 1, device=x.device)\n",
        "\n",
        "        E_kin = 0.5 * m_pos * acc_mag\n",
        "\n",
        "        # gyro proxy\n",
        "        if len(self.gyro_indices) > 0:\n",
        "            gyro_data = x_t[:, :, self.gyro_indices]\n",
        "            n_gyro = len(self.gyro_indices) // 3\n",
        "            if n_gyro > 0:\n",
        "                gyro_reshaped = gyro_data.view(B, T, n_gyro, 3)\n",
        "                gyro_mag = (gyro_reshaped ** 2).sum(dim=-1).mean(dim=-1, keepdim=True)  # (B,T,1)\n",
        "            else:\n",
        "                gyro_mag = torch.zeros(B, T, 1, device=x.device)\n",
        "        else:\n",
        "            gyro_mag = torch.zeros(B, T, 1, device=x.device)\n",
        "\n",
        "        E_rot = 0.5 * I_pos * gyro_mag\n",
        "        return E_kin + E_rot  # (B,T,1)\n",
        "\n",
        "\n",
        "class PotentialEnergyField(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.energy_net = nn.Sequential(\n",
        "            nn.Linear(input_dim + 1, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, E_phys):  # x:(B,C,T), E_phys:(B,T,1)\n",
        "        x_t = x.transpose(1, 2)                # (B,T,C)\n",
        "        x_aug = torch.cat([x_t, E_phys], -1)   # (B,T,C+1)\n",
        "        return self.energy_net(x_aug)          # (B,T,1)\n",
        "\n",
        "\n",
        "class EnergyGradientFlow(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.gradient_net = nn.Sequential(\n",
        "            nn.Linear(input_dim + 1, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, E_phys):  # x:(B,C,T), E_phys:(B,T,1)\n",
        "        x_t = x.transpose(1, 2)                # (B,T,C)\n",
        "        x_aug = torch.cat([x_t, E_phys], -1)   # (B,T,C+1)\n",
        "        return self.gradient_net(x_aug)        # (B,T,C)\n",
        "\n",
        "\n",
        "class RateAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.rate_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.rate_gate = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, features, energy):  # features:(B,T,H), energy:(B,T,1)\n",
        "        energy_rate = torch.cat([energy[:, :1, :], energy[:, 1:, :] - energy[:, :-1, :]], dim=1)\n",
        "        ctx = self.rate_proj(features)\n",
        "        scores = self.rate_gate(ctx + energy_rate)\n",
        "        attn = torch.softmax(scores.squeeze(-1), dim=1)\n",
        "        return attn, features * attn.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.phase_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.phase_gate = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, features, energy):\n",
        "        energy_pad = F.pad(energy.transpose(1, 2), (1, 1), mode='replicate').transpose(1, 2)\n",
        "        curv = energy_pad[:, 2:, :] - 2 * energy_pad[:, 1:-1, :] + energy_pad[:, :-2, :]\n",
        "        ctx = self.phase_proj(features)\n",
        "        scores = self.phase_gate(ctx + curv)\n",
        "        attn = torch.softmax(scores.squeeze(-1), dim=1)\n",
        "        return attn, features * attn.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class AttentionToAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.meta_attention = nn.Sequential(\n",
        "            nn.Linear(2, hidden_dim // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim // 4, 2),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.conflict_detector = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, rate_attn, phase_attn, rate_feat, phase_feat):\n",
        "        attn_stack = torch.stack([rate_attn, phase_attn], dim=-1)  # (B,T,2)\n",
        "        reliability = self.meta_attention(attn_stack)              # (B,T,2)\n",
        "        rate_reliability = reliability[:, :, 0:1]\n",
        "        phase_reliability = reliability[:, :, 1:2]\n",
        "\n",
        "        feat_cat = torch.cat([rate_feat, phase_feat], dim=-1)\n",
        "        conflict_score = self.conflict_detector(feat_cat).mean(dim=1).squeeze(-1)\n",
        "\n",
        "        reconciled = rate_reliability * rate_feat + phase_reliability * phase_feat\n",
        "        return reconciled, conflict_score\n",
        "\n",
        "\n",
        "class LandscapeGeometryEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        combined_dim = input_dim * 2 + 1\n",
        "        self.input_proj = nn.Conv1d(combined_dim, hidden_dim, kernel_size=1)\n",
        "        self.conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.temporal_attn = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
        "\n",
        "        self.rate_attention = RateAttention(hidden_dim)\n",
        "        self.phase_attention = PhaseAttention(hidden_dim)\n",
        "        self.a2 = AttentionToAttention(hidden_dim)\n",
        "\n",
        "    def forward(self, energy, gradient, x_original):\n",
        "        x_t = x_original.transpose(1, 2)                     # (B,T,C)\n",
        "        state = torch.cat([x_t, gradient, energy], dim=-1)   # (B,T,2C+1)\n",
        "\n",
        "        h = self.input_proj(state.transpose(1, 2))           # (B,H,T)\n",
        "        h = self.conv(h)\n",
        "        h = self.norm(h.transpose(1, 2)).transpose(1, 2)\n",
        "        h = F.gelu(h)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        h_t = h.transpose(1, 2)                              # (B,T,H)\n",
        "        h_t, _ = self.temporal_attn(h_t, h_t, h_t)\n",
        "\n",
        "        rate_attn, rate_feat = self.rate_attention(h_t, energy)\n",
        "        phase_attn, phase_feat = self.phase_attention(h_t, energy)\n",
        "        reconciled, conflict = self.a2(rate_attn, phase_attn, rate_feat, phase_feat)\n",
        "        return reconciled, conflict\n",
        "\n",
        "\n",
        "class MIELHAR_A2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, sensor_config,\n",
        "                 physics_grad_weight=0.1, energy_reg_weight=0.01):\n",
        "        super().__init__()\n",
        "        self.acc_indices = sensor_config[\"acc_indices\"]\n",
        "        self.gyro_indices = sensor_config[\"gyro_indices\"]\n",
        "\n",
        "        self.physics = RelativeEnergyPhysics(self.acc_indices, self.gyro_indices)\n",
        "        self.potential_field = PotentialEnergyField(input_dim, hidden_dim)\n",
        "        self.gradient_flow   = EnergyGradientFlow(input_dim, hidden_dim)\n",
        "        self.physics_grad_weight = physics_grad_weight\n",
        "\n",
        "        self.encoder = LandscapeGeometryEncoder(input_dim, hidden_dim)\n",
        "        self.energy_reg_weight = energy_reg_weight\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def _physics_grad(self, x):\n",
        "        x_t = x.transpose(1, 2)  # (B,T,C)\n",
        "        m_pos = F.softplus(self.physics.m)\n",
        "        I_pos = F.softplus(self.physics.I)\n",
        "\n",
        "        pg = torch.zeros_like(x_t)\n",
        "        if len(self.acc_indices) > 0:\n",
        "            pg[:, :, self.acc_indices] = x_t[:, :, self.acc_indices] * m_pos\n",
        "        if len(self.gyro_indices) > 0:\n",
        "            pg[:, :, self.gyro_indices] = x_t[:, :, self.gyro_indices] * I_pos\n",
        "        return pg\n",
        "\n",
        "    def forward(self, x, return_energy=False):\n",
        "        E_phys = self.physics(x)  # (B,T,1)\n",
        "\n",
        "        energy = self.potential_field(x, E_phys)        # (B,T,1)\n",
        "        grad   = self.gradient_flow(x, E_phys)          # (B,T,C)\n",
        "        grad = grad + self.physics_grad_weight * self._physics_grad(x)\n",
        "\n",
        "        landscape_features, _ = self.encoder(energy, grad, x)  # (B,T,H)\n",
        "\n",
        "        g = self.pool(landscape_features.transpose(1, 2)).squeeze(-1)  # (B,H)\n",
        "        logits = self.classifier(g)\n",
        "\n",
        "        if return_energy:\n",
        "            return logits, energy, grad\n",
        "        return logits\n",
        "\n",
        "\n",
        "def compute_energy_loss(energy, gradient, beta_gradmag=0.1):\n",
        "    energy_diff = energy[:, 1:] - energy[:, :-1]\n",
        "    smoothness = torch.mean(energy_diff ** 2)\n",
        "    grad_mag = torch.norm(gradient, dim=-1)\n",
        "    grad_loss = torch.mean(grad_mag)\n",
        "    return smoothness + beta_gradmag * grad_loss\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3) Train/Eval\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, beta_gradmag=0.1):\n",
        "    model.train()\n",
        "\n",
        "    total_sum = 0.0\n",
        "    cls_sum = 0.0\n",
        "    e_sum = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        logits, energy, grad = model(x, return_energy=True)\n",
        "        cls_loss = criterion(logits, y)\n",
        "        e_loss = compute_energy_loss(energy, grad, beta_gradmag=beta_gradmag)\n",
        "\n",
        "        loss = cls_loss + model.energy_reg_weight * e_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_sum += loss.item()\n",
        "        cls_sum += cls_loss.item()\n",
        "        e_sum += e_loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return {\n",
        "        \"loss_total\": total_sum / max(1, n_batches),\n",
        "        \"loss_cls\": cls_sum / max(1, n_batches),\n",
        "        \"loss_energy\": e_sum / max(1, n_batches),\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_macro_f1(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(y.numpy())\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    return f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4) Main\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    CFG = {\n",
        "        \"seed\": 42,\n",
        "        \"deterministic\": True,\n",
        "\n",
        "        \"dataset_root\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/UCI_HAR\",\n",
        "\n",
        "        \"batch_size\": 512,\n",
        "        \"num_workers\": 2,\n",
        "        \"pin_memory\": True,\n",
        "\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 8e-4,\n",
        "        \"weight_decay\": 3e-4,\n",
        "        \"label_smoothing\": 0.05,\n",
        "        \"physics_grad_weight\": 0.25,  # EnergyGradientFlow: grad + alpha * physics_grad_weight\n",
        "\n",
        "        \"acc_indices\": [0, 1, 2],\n",
        "        \"gyro_indices\": [6, 7, 8],\n",
        "\n",
        "        # loss weights\n",
        "        \"w_energy_reg\": 0.1,      # multiply energy loss\n",
        "        \"beta_gradmag\": 0.2,      # compute_energy_loss: smooth + beta * gradmag\n",
        "\n",
        "        # model\n",
        "        \"hidden_dim\": 64,\n",
        "        \"num_classes\": 6,\n",
        "        \"input_dim\": 9,\n",
        "\n",
        "        # logging\n",
        "        \"log_every\": 10,\n",
        "    }\n",
        "\n",
        "    set_seed(CFG[\"seed\"], CFG[\"deterministic\"])\n",
        "    seed_worker = make_seed_worker(CFG[\"seed\"])\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(CFG[\"seed\"])\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"MIEL-HAR with Attention-to-Attention (A²)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Device: {device}\\n\")\n",
        "\n",
        "    # 1) Load official splits\n",
        "    train_raw = UCIHARDataset(CFG[\"dataset_root\"], split=\"train\")\n",
        "    test_raw  = UCIHARDataset(CFG[\"dataset_root\"], split=\"test\")\n",
        "\n",
        "    X_train = np.transpose(train_raw.X, (0, 2, 1)).astype(np.float32)  # (N,C,T)\n",
        "    y_train = train_raw.y.astype(np.int64)\n",
        "\n",
        "    X_test = np.transpose(test_raw.X, (0, 2, 1)).astype(np.float32)\n",
        "    y_test = test_raw.y.astype(np.int64)\n",
        "\n",
        "    # 2) Standardize train-only\n",
        "    X_train_s, X_test_s = fit_transform_scaler_train_only(X_train, X_test)\n",
        "\n",
        "    train_ds = ArrayDataset(X_train_s, y_train)\n",
        "    test_ds  = ArrayDataset(X_test_s, y_test)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=CFG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=CFG[\"num_workers\"],\n",
        "        pin_memory=CFG[\"pin_memory\"] and (device.type == \"cuda\"),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        persistent_workers=(CFG[\"num_workers\"] > 0),\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=CFG[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=CFG[\"num_workers\"],\n",
        "        pin_memory=CFG[\"pin_memory\"] and (device.type == \"cuda\"),\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        persistent_workers=(CFG[\"num_workers\"] > 0),\n",
        "    )\n",
        "\n",
        "    sensor_config = {\"acc_indices\": CFG[\"acc_indices\"], \"gyro_indices\": CFG[\"gyro_indices\"]}\n",
        "\n",
        "    model = MIELHAR_A2(\n",
        "        input_dim=CFG[\"input_dim\"],\n",
        "        hidden_dim=CFG[\"hidden_dim\"],\n",
        "        num_classes=CFG[\"num_classes\"],\n",
        "        sensor_config=sensor_config,\n",
        "        physics_grad_weight=CFG[\"physics_grad_weight\"],\n",
        "        energy_reg_weight=CFG[\"w_energy_reg\"],\n",
        "    ).to(device)\n",
        "\n",
        "    n_params = count_params(model, trainable_only=True)\n",
        "    print(f\"Parameters: {n_params:,} ({n_params/1e6:.3f} M)\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
        "\n",
        "    warmup_epochs = 5\n",
        "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=0.1,\n",
        "        end_factor=1.0,\n",
        "        total_iters=warmup_epochs\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=CFG[\"label_smoothing\"])\n",
        "\n",
        "    best_test_f1 = -1.0\n",
        "    best_state = None\n",
        "    best_epoch = -1\n",
        "\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"{'Epoch':<8} {'LR':<10} {'Total':<10} {'Cls':<10} {'E':<10} {'TestF1(macro)':<14} {'Best':<6}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for epoch in range(1, CFG[\"epochs\"] + 1):\n",
        "        train_logs = train_one_epoch(\n",
        "            model=model,\n",
        "            loader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            beta_gradmag=CFG[\"beta_gradmag\"],\n",
        "        )\n",
        "\n",
        "        if epoch <= warmup_epochs:\n",
        "            warmup_scheduler.step()\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        test_f1 = eval_macro_f1(model, test_loader, device)\n",
        "\n",
        "        is_best = \"\"\n",
        "        if test_f1 > best_test_f1 + 1e-12:\n",
        "            best_test_f1 = test_f1\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            best_epoch = epoch\n",
        "            is_best = \"*\"\n",
        "\n",
        "        if (epoch % CFG[\"log_every\"]) == 0:\n",
        "            print(f\"{epoch:<8} {lr:<10.6f} \"\n",
        "                  f\"{train_logs['loss_total']:<10.4f} {train_logs['loss_cls']:<10.4f} {train_logs['loss_energy']:<10.4f} \"\n",
        "                  f\"{test_f1:<14.4f} {is_best:<6}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Best Test F1(macro): {best_test_f1:.4f} @ epoch {best_epoch}\")\n",
        "\n",
        "    # final confirm\n",
        "    model.load_state_dict(best_state)\n",
        "    final_test_f1 = eval_macro_f1(model, test_loader, device)\n",
        "\n",
        "    print(\"\\nFinal Test:\")\n",
        "    print(f\"Test F1(macro): {final_test_f1:.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}